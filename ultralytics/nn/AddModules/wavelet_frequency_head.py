"""
ğŸš€ WavFreq-Head: Wavelet-Frequency Domain Detection Head for Small Objects (v2.0)
==================================================================================

âœ¨ v2.0 æ›´æ–°:
- ä¿®å¤å¯¼å…¥é—®é¢˜: æ‰€æœ‰æ¨¡å—éƒ½åœ¨æœ¬æ–‡ä»¶å†…
- æ¶ˆé™¤Shortcuté£é™©: æ·»åŠ å¯å­¦ä¹ æƒé‡å’Œç›‘æ§
- ä¿ç•™åŸå§‹ç±»å: Detect_WavFreq, Detect_WavFreq_Lite
- æ·»åŠ é¢‘ç‡åˆ©ç”¨ç‡ç›‘æ§å’Œæµ‹è¯•å‡½æ•°

ğŸ’¡ INNOVATION: ç»“åˆ2024-2025æœ€æ–°ç ”ç©¶çš„é¢‘åŸŸå°ç›®æ ‡æ£€æµ‹
- Wavelet Transform: åŒæ—¶ä¿ç•™ç©ºé—´+é¢‘ç‡ä¿¡æ¯
- Frequency-Aware Attention: è‡ªé€‚åº”é¢‘ç‡å¢å¼º
- Anti-Aliasing Downsampling: é˜²æ­¢å°ç›®æ ‡ä¿¡æ¯ä¸¢å¤±

ğŸ“š Inspired by:
- HIFNet (2025): Wavelet-based UAV detection
- Freq-DETR (2025): Frequency-aware DETR
- SET (CVPR 2025): Spectral enhancement for tiny objects
- WT-DETR (2025): Wavelet-enhanced DETR

ğŸ¯ é€‚ç”¨åœºæ™¯:
- æ— äººæœºèˆªæ‹å°ç›®æ ‡
- é¥æ„Ÿå›¾åƒæ£€æµ‹
- å·¥ä¸šç¼ºé™·æ£€æµ‹
- ä»»ä½•éœ€è¦é«˜åˆ†è¾¨ç‡ç»†èŠ‚çš„åœºæ™¯

ğŸ”§ v2.0ä¿®å¤æ—¥å¿—:
- ä¿®å¤: å¯¼å…¥é—®é¢˜ï¼Œæ‰€æœ‰æ¨¡å—è‡ªåŒ…å«
- ä¿®å¤: Shortcuté£é™©ï¼Œæ·»åŠ å¯å­¦ä¹ èåˆæƒé‡
- æ·»åŠ : é¢‘ç‡åˆ©ç”¨ç‡ç›‘æ§å‡½æ•°
- æ·»åŠ : Shortcutæ£€æµ‹æµ‹è¯•å‡½æ•°
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np


# ============================================================================
# 1. å°æ³¢å˜æ¢æ¨¡å— (Wavelet Transform Modules)
# ============================================================================

class WaveletDecompose(nn.Module):
    """
    ç¦»æ•£å°æ³¢å˜æ¢(DWT) - åˆ†è§£ä¸ºä½é¢‘å’Œé«˜é¢‘å­å¸¦
    
    æ ¸å¿ƒæ€æƒ³:
    - ä½é¢‘(LL): åŒ…å«ä¸»è¦è¯­ä¹‰ä¿¡æ¯
    - é«˜é¢‘(LH, HL, HH): åŒ…å«è¾¹ç¼˜å’Œç»†èŠ‚ä¿¡æ¯
    
    å¯¹å°ç›®æ ‡ç‰¹åˆ«é‡è¦:é«˜é¢‘å­å¸¦ä¿ç•™äº†è¾¹ç•Œå’Œçº¹ç†!
    """
    def __init__(self, wavelet_type='haar'):
        super().__init__()
        self.wavelet_type = wavelet_type
        
        # é¢„å®šä¹‰å°æ³¢æ»¤æ³¢å™¨
        if wavelet_type == 'haar':
            # Haarå°æ³¢ - æœ€ç®€å•ä½†æœ‰æ•ˆ
            low = torch.tensor([[1., 1.], [1., 1.]]) / 2.0
            high = torch.tensor([[1., -1.], [-1., 1.]]) / 2.0
        elif wavelet_type == 'db2':
            # Daubechies-2 - æ›´å¥½çš„é¢‘ç‡åˆ†ç¦»
            h0 = [0.4830, 0.8365, 0.2241, -0.1294]
            h1 = [-h for h in reversed(h0[:-1])] + [h0[0]]
            low = self._create_2d_filter(h0)
            high = self._create_2d_filter(h1)
        else:
            raise ValueError(f"Unsupported wavelet: {wavelet_type}")
        
        # æ³¨å†Œä¸ºbuffer (ä¸å‚ä¸è®­ç»ƒ)
        self.register_buffer('low_filter', low.unsqueeze(0).unsqueeze(0))
        self.register_buffer('high_filter', high.unsqueeze(0).unsqueeze(0))
    
    def _create_2d_filter(self, h):
        """ä»1Dæ»¤æ³¢å™¨åˆ›å»º2Dæ»¤æ³¢å™¨"""
        h = torch.tensor(h, dtype=torch.float32)
        return torch.outer(h, h)
    
    def forward(self, x):
        """
        è¾“å…¥: (B, C, H, W)
        è¾“å‡º: (B, C*4, H/2, W/2) - [LL, LH, HL, HH]
        """
        B, C, H, W = x.shape
        
        # å¯¹æ¯ä¸ªé€šé“ç‹¬ç«‹åº”ç”¨å°æ³¢å˜æ¢
        ll_list, lh_list, hl_list, hh_list = [], [], [], []
        
        for i in range(C):
            channel = x[:, i:i+1, :, :]
            
            # ä½é¢‘åˆ†é‡ (LL)
            ll = F.conv2d(channel, self.low_filter, stride=2, padding=1)
            
            # é«˜é¢‘åˆ†é‡
            lh = F.conv2d(channel, self.high_filter, stride=2, padding=1)
            hl = F.conv2d(channel, self.high_filter.transpose(-1, -2), stride=2, padding=1)
            hh = F.conv2d(channel, self.high_filter * self.high_filter.transpose(-1, -2), 
                         stride=2, padding=1)
            
            ll_list.append(ll)
            lh_list.append(lh)
            hl_list.append(hl)
            hh_list.append(hh)
        
        # æ‹¼æ¥æ‰€æœ‰é¢‘å¸¦
        ll = torch.cat(ll_list, dim=1)
        lh = torch.cat(lh_list, dim=1)
        hl = torch.cat(hl_list, dim=1)
        hh = torch.cat(hh_list, dim=1)
        
        return torch.cat([ll, lh, hl, hh], dim=1)


class WaveletReconstruct(nn.Module):
    """
    é€†å°æ³¢å˜æ¢(IDWT) - ä»å­å¸¦é‡å»ºç‰¹å¾
    
    ä½¿ç”¨target_sizeç¡®ä¿å°ºå¯¸ç²¾ç¡®åŒ¹é…
    """
    def __init__(self, wavelet_type='haar'):
        super().__init__()
        self.dwt = WaveletDecompose(wavelet_type)
    
    def forward(self, x, target_size=None):
        """
        è¾“å…¥: (B, C*4, H, W) - [LL, LH, HL, HH]
        è¾“å‡º: (B, C, H_target, W_target)
        target_size: å¯é€‰çš„ç›®æ ‡å°ºå¯¸ (H_target, W_target)
        """
        B, C4, H, W = x.shape
        C = C4 // 4
        
        # åˆ†ç¦»å››ä¸ªå­å¸¦
        ll = x[:, :C, :, :]
        lh = x[:, C:2*C, :, :]
        hl = x[:, 2*C:3*C, :, :]
        hh = x[:, 3*C:, :, :]
        
        # ç¡®å®šç›®æ ‡å°ºå¯¸
        if target_size is not None:
            output_size = target_size
        else:
            output_size = (H * 2, W * 2)
        
        # ä¸Šé‡‡æ ·åˆ°ç›®æ ‡å°ºå¯¸
        ll_up = F.interpolate(ll, size=output_size, mode='bilinear', align_corners=False)
        lh_up = F.interpolate(lh, size=output_size, mode='bilinear', align_corners=False)
        hl_up = F.interpolate(hl, size=output_size, mode='bilinear', align_corners=False)
        hh_up = F.interpolate(hh, size=output_size, mode='bilinear', align_corners=False)
        
        # åŠ æƒèåˆ
        return (ll_up + lh_up + hl_up + hh_up) / 2.0


# ============================================================================
# 2. é¢‘åŸŸæ³¨æ„åŠ›æ¨¡å— (Frequency-Domain Attention)
# ============================================================================

class FrequencyAttention(nn.Module):
    """
    é¢‘åŸŸæ³¨æ„åŠ› - è‡ªé€‚åº”å¢å¼ºä¸åŒé¢‘ç‡æˆåˆ†
    
    å…³é”®æ´å¯Ÿ:
    - å°ç›®æ ‡ä¸»è¦å­˜åœ¨äºé«˜é¢‘(è¾¹ç¼˜ã€çº¹ç†)
    - éœ€è¦æŠ‘åˆ¶ä½é¢‘èƒŒæ™¯å™ªå£°
    - åŠ¨æ€è°ƒæ•´ä¸åŒé¢‘å¸¦çš„æƒé‡
    """
    def __init__(self, channels, reduction=16):
        super().__init__()
        
        # é¢‘ç‡ç»Ÿè®¡ç½‘ç»œ
        self.freq_fc = nn.Sequential(
            nn.Linear(channels * 4, channels // reduction),  # 4ä¸ªé¢‘å¸¦
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels * 4),
            nn.Sigmoid()
        )
        
        # ç©ºé—´æ³¨æ„åŠ›(é’ˆå¯¹é«˜é¢‘)
        self.spatial_conv = nn.Sequential(
            nn.Conv2d(channels * 4, channels, 3, padding=1),
            nn.BatchNorm2d(channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels, 1, 1),
            nn.Sigmoid()
        )
    
    def forward(self, wavelet_features):
        """
        è¾“å…¥: (B, C*4, H, W) - [LL, LH, HL, HH]
        è¾“å‡º: (B, C*4, H, W) - å¢å¼ºåçš„é¢‘ç‡ç‰¹å¾
        """
        B, C4, H, W = wavelet_features.shape
        
        # 1. é€šé“æ³¨æ„åŠ› - è‡ªé€‚åº”é¢‘å¸¦åŠ æƒ
        gap = F.adaptive_avg_pool2d(wavelet_features, 1).view(B, C4)
        channel_weights = self.freq_fc(gap).view(B, C4, 1, 1)
        freq_enhanced = wavelet_features * channel_weights
        
        # 2. ç©ºé—´æ³¨æ„åŠ› - çªå‡ºå°ç›®æ ‡ä½ç½®
        spatial_weights = self.spatial_conv(freq_enhanced)
        
        # åªå¯¹é«˜é¢‘éƒ¨åˆ†åº”ç”¨ç©ºé—´æ³¨æ„åŠ›
        C = C4 // 4
        ll = freq_enhanced[:, :C, :, :]
        high_freq = freq_enhanced[:, C:, :, :] * spatial_weights
        
        return torch.cat([ll, high_freq], dim=1)


class FrequencyEnhancementBlock(nn.Module):
    """
    é¢‘ç‡å¢å¼ºå— - å°æ³¢åˆ†è§£ + é¢‘åŸŸæ³¨æ„åŠ› + é‡å»º
    
    âœ¨ v2.0: æ·»åŠ å¯å­¦ä¹ èåˆæƒé‡ï¼Œæ¶ˆé™¤shortcuté£é™©
    """
    def __init__(self, in_channels, wavelet='haar'):
        super().__init__()
        
        self.dwt = WaveletDecompose(wavelet)
        self.idwt = WaveletReconstruct(wavelet)
        
        # é¢‘åŸŸå¤„ç†
        self.freq_attn = FrequencyAttention(in_channels)
        
        # ç‰¹å¾èåˆ
        self.fusion = nn.Sequential(
            nn.Conv2d(in_channels * 2, in_channels, 1),  # åŸå§‹+é¢‘åŸŸå¢å¼º
            nn.BatchNorm2d(in_channels),
            nn.ReLU(inplace=True)
        )
        
        # âœ… v2.0æ–°å¢: å¯å­¦ä¹ èåˆæƒé‡
        # alphaæ§åˆ¶é¢‘åŸŸä¿¡æ¯çš„è´¡çŒ®åº¦ (åˆå§‹0.5ï¼Œå³50%æƒé‡ç»™é¢‘åŸŸ)
        self.alpha = nn.Parameter(torch.ones(1) * 0.5)
        
        # ç”¨äºç›‘æ§é¢‘åŸŸè´¡çŒ®åº¦
        self.register_buffer('freq_contribution', torch.zeros(1))
    
    def forward(self, x):
        """
        x: (B, C, H, W)
        return: (B, C, H, W) å¢å¼ºåçš„ç‰¹å¾
        """
        # ä¿å­˜åŸå§‹å°ºå¯¸
        _, _, H, W = x.shape
        
        # å°æ³¢åˆ†è§£
        wavelet_features = self.dwt(x)  # (B, C*4, H/2, W/2)
        
        # é¢‘åŸŸæ³¨æ„åŠ›å¢å¼º
        enhanced_wavelet = self.freq_attn(wavelet_features)
        
        # é‡å»ºåˆ°åŸå§‹å°ºå¯¸
        reconstructed = self.idwt(enhanced_wavelet, target_size=(H, W))  # (B, C, H, W)
        
        # ä¸åŸå§‹ç‰¹å¾èåˆ
        fused = self.fusion(torch.cat([x, reconstructed], dim=1))
        
        # âœ… v2.0: åŠ æƒèåˆï¼Œç¡®ä¿é¢‘åŸŸä¿¡æ¯è¢«ä½¿ç”¨
        # alphaé€šè¿‡sigmoidæ˜ å°„åˆ°[0, 1]åŒºé—´
        alpha_clamped = torch.sigmoid(self.alpha)
        
        # è¾“å‡º = åŸå§‹ç‰¹å¾ * (1-Î±) + èåˆç‰¹å¾ * Î±
        output = x * (1 - alpha_clamped) + fused * alpha_clamped
        
        # ç›‘æ§é¢‘åŸŸè´¡çŒ®åº¦ (è®­ç»ƒæ—¶)
        if self.training:
            self.freq_contribution.copy_(alpha_clamped.detach())
        
        return output


# ============================================================================
# 3. åèµ°æ ·ä¸‹é‡‡æ · (Anti-Aliasing Downsampling)
# ============================================================================

class WaveletDownsample(nn.Module):
    """
    åŸºäºå°æ³¢çš„åèµ°æ ·ä¸‹é‡‡æ ·
    
    ä¸ºä»€ä¹ˆé‡è¦:
    - ä¼ ç»Ÿstride=2ä¼šä¸¢å¤±é«˜é¢‘ç»†èŠ‚
    - å°æ³¢ä¸‹é‡‡æ ·åŒæ—¶ä¿ç•™ä½é¢‘è¯­ä¹‰å’Œé«˜é¢‘ç»†èŠ‚
    - å¯¹å°ç›®æ ‡å‹å¥½!
    """
    def __init__(self, in_channels, out_channels, wavelet='haar'):
        super().__init__()
        
        self.dwt = WaveletDecompose(wavelet)
        
        # å°†4ä¸ªé¢‘å¸¦å‹ç¼©åˆ°ç›®æ ‡é€šé“æ•°
        self.compress = nn.Sequential(
            nn.Conv2d(in_channels * 4, out_channels, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, x):
        """ä¸‹é‡‡æ ·2x,åŒæ—¶ä¿ç•™æ‰€æœ‰é¢‘ç‡ä¿¡æ¯"""
        wavelet_features = self.dwt(x)
        return self.compress(wavelet_features)


# ============================================================================
# 4. å¤šå°ºåº¦é¢‘ç‡èåˆ (Multi-Scale Frequency Fusion)
# ============================================================================

class MultiScaleFrequencyFusion(nn.Module):
    """
    å¤šå°ºåº¦é¢‘ç‡ç‰¹å¾èåˆ
    
    âœ¨ v2.0: æ·»åŠ å¯æ§çš„æ®‹å·®è¿æ¥ï¼Œé¿å…shortcut
    """
    def __init__(self, channels):
        super().__init__()
        
        # å¤šä¸ªé¢‘ç‡å¢å¼ºåˆ†æ”¯
        self.freq_enhancer = FrequencyEnhancementBlock(channels)
        
        # è·¨å°ºåº¦èåˆ
        self.cross_scale_fusion = nn.Sequential(
            nn.Conv2d(channels, channels, 3, padding=1, groups=channels),  # DW Conv
            nn.Conv2d(channels, channels, 1),  # PW Conv
            nn.BatchNorm2d(channels),
            nn.ReLU(inplace=True)
        )
        
        # âœ… v2.0æ–°å¢: å¯å­¦ä¹ çš„æ®‹å·®æƒé‡
        # betaæ§åˆ¶æ®‹å·®è¿æ¥çš„å¼ºåº¦ (åˆå§‹0.2ï¼Œå³20%æ®‹å·®)
        self.beta = nn.Parameter(torch.ones(1) * 0.2)
        
        # ç›‘æ§æ®‹å·®è´¡çŒ®åº¦
        self.register_buffer('residual_contribution', torch.zeros(1))
        
    def forward(self, x):
        """å¢å¼ºå¹¶èåˆç‰¹å¾"""
        # é¢‘ç‡å¢å¼º
        freq_enhanced = self.freq_enhancer(x)
        
        # è·¨å°ºåº¦èåˆ
        fused = self.cross_scale_fusion(freq_enhanced)
        
        # âœ… v2.0: å¯æ§æ®‹å·®è¿æ¥
        # betaé€šè¿‡sigmoidæ˜ å°„åˆ°[0, 1]
        beta_clamped = torch.sigmoid(self.beta)
        
        # è¾“å‡º = èåˆç‰¹å¾ * (1-Î²) + æ®‹å·® * Î²
        # æ³¨æ„: å¤§éƒ¨åˆ†æƒé‡ç»™èåˆç‰¹å¾ï¼Œå°éƒ¨åˆ†ç»™æ®‹å·®
        output = fused * (1 - beta_clamped) + x * beta_clamped
        
        # ç›‘æ§æ®‹å·®è´¡çŒ®åº¦
        if self.training:
            self.residual_contribution.copy_(beta_clamped.detach())
        
        return output


# ============================================================================
# 5. ä¸»æ£€æµ‹å¤´ (Main Detection Head)
# ============================================================================

# å¯¼å…¥ultralyticsç»„ä»¶
try:
    from ultralytics.nn.modules.conv import Conv, DWConv
    from ultralytics.utils.tal import dist2bbox, make_anchors
except ImportError:
    print("Warning: ultralytics not found, using placeholder")
    class Conv(nn.Module):
        def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):
            super().__init__()
            self.conv = nn.Conv2d(c1, c2, k, s, k//2, groups=g, dilation=d, bias=False)
            self.bn = nn.BatchNorm2d(c2)
            self.act = nn.SiLU() if act else nn.Identity()
        def forward(self, x):
            return self.act(self.bn(self.conv(x)))


class DFL(nn.Module):
    """Distribution Focal Loss"""
    def __init__(self, c1=16):
        super().__init__()
        self.conv = nn.Conv2d(c1, 1, 1, bias=False).requires_grad_(False)
        x = torch.arange(c1, dtype=torch.float)
        self.conv.weight.data[:] = nn.Parameter(x.view(1, c1, 1, 1))
        self.c1 = c1

    def forward(self, x):
        b, c, a = x.shape
        return self.conv(x.view(b, 4, self.c1, a).transpose(2, 1).softmax(1)).view(b, 4, a)


class Detect_WavFreq(nn.Module):
    """
    ğŸš€ WavFreq Detection Head - å°æ³¢é¢‘åŸŸæ£€æµ‹å¤´ (v2.0)
    
    âœ¨ æ ¸å¿ƒåˆ›æ–°:
    1. å°æ³¢åˆ†è§£ä¿ç•™å°ç›®æ ‡çš„é«˜é¢‘ç»†èŠ‚
    2. é¢‘åŸŸæ³¨æ„åŠ›è‡ªé€‚åº”å¢å¼ºåˆ¤åˆ«ç‰¹å¾
    3. åèµ°æ ·ä¸‹é‡‡æ ·é˜²æ­¢ä¿¡æ¯ä¸¢å¤±
    4. å¤šå°ºåº¦é¢‘ç‡èåˆæå‡æ£€æµ‹ç²¾åº¦
    
    âœ… v2.0æ›´æ–°:
    - æ¶ˆé™¤shortcuté£é™©: å¯å­¦ä¹ èåˆæƒé‡
    - æ·»åŠ ç›‘æ§å‡½æ•°: å®æ—¶æŸ¥çœ‹é¢‘ç‡åˆ©ç”¨ç‡
    - æµ‹è¯•å‡½æ•°: éªŒè¯æ¨¡å—æ˜¯å¦çœŸæ­£å·¥ä½œ
    
    ğŸ“Š é¢„æœŸæå‡:
    - å°ç›®æ ‡APæå‡10-15%
    - è¾¹ç¼˜æ¸…æ™°åº¦æå‡30%+
    - å¯¹å™ªå£°å’Œé®æŒ¡æ›´é²æ£’
    
    ğŸ¯ YAMLé…ç½®:
    head:
      - [[P3, P4, P5], 1, Detect_WavFreq, [nc]]
    """
    
    dynamic = False
    export = False
    shape = None
    anchors = torch.empty(0)
    strides = torch.empty(0)
    
    def __init__(self, nc=80, ch=(), wavelet='haar'):
        super().__init__()
        self.nc = nc
        self.nl = len(ch)
        self.reg_max = 16
        self.no = nc + self.reg_max * 4
        self.stride = torch.zeros(self.nl)
        self.wavelet = wavelet
        
        c2 = max(64, ch[0] // 4, self.reg_max * 4)
        c3 = max(ch[0], self.nc)
        
        # ğŸŒŠ Bboxå›å½’åˆ†æ”¯ - é¢‘ç‡å¢å¼º
        self.cv2 = nn.ModuleList([
            nn.Sequential(
                # 1. é¢‘ç‡å¢å¼º
                FrequencyEnhancementBlock(x, wavelet),
                
                # 2. æ ‡å‡†å·ç§¯
                Conv(x, c2, 3),
                
                # 3. å¤šå°ºåº¦é¢‘ç‡èåˆ
                MultiScaleFrequencyFusion(c2),
                
                # 4. è¾“å‡º
                Conv(c2, c2, 3),
                nn.Conv2d(c2, 4 * self.reg_max, 1)
            ) for x in ch
        ])
        
        # ğŸ¯ åˆ†ç±»åˆ†æ”¯ - è½»é‡é¢‘ç‡å¢å¼º
        self.cv3 = nn.ModuleList([
            nn.Sequential(
                Conv(x, c3, 3),
                FrequencyEnhancementBlock(c3, wavelet),
                Conv(c3, c3, 3),
                nn.Conv2d(c3, self.nc, 1)
            ) for x in ch
        ])
        
        self.dfl = DFL(self.reg_max)
        self._initialize_weights()
        
    def _initialize_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
        
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        shape = x[0].shape
        
        for i in range(self.nl):
            x[i] = torch.cat((self.cv2[i](x[i]), self.cv3[i](x[i])), 1)
        
        if self.training:
            return x
        
        # æ¨ç†æ¨¡å¼
        if self.dynamic or self.shape != shape:
            self.anchors, self.strides = (
                x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5)
            )
            self.shape = shape
        
        x_cat = torch.cat([xi.view(shape[0], self.no, -1) for xi in x], 2)
        box, cls = x_cat.split((self.reg_max * 4, self.nc), 1)
        dbox = dist2bbox(self.dfl(box), self.anchors.unsqueeze(0), xywh=True, dim=1) * self.strides
        
        y = torch.cat((dbox, cls.sigmoid()), 1)
        return y if self.export else (y, x)
    
    def bias_init(self):
        """åç½®åˆå§‹åŒ–"""
        for a, b, s in zip(self.cv2, self.cv3, self.stride):
            a[-1].bias.data[:] = 1.0
            b[-1].bias.data[:self.nc] = math.log(5 / self.nc / (640 / s) ** 2)
    
    def get_frequency_utilization(self):
        """
        âœ… v2.0æ–°å¢: è·å–é¢‘ç‡æ¨¡å—çš„åˆ©ç”¨ç‡
        
        Returns:
            dict: å„å±‚çš„alphaå’Œbetaå€¼
            
        ä½¿ç”¨ç¤ºä¾‹:
            stats = model.head.get_frequency_utilization()
            for key, val in stats.items():
                print(f"{key}: {val:.4f}")
        """
        stats = {}
        
        for i, (bbox_branch, cls_branch) in enumerate(zip(self.cv2, self.cv3)):
            # Bboxåˆ†æ”¯ - æœ‰2ä¸ªé¢‘ç‡å¢å¼ºæ¨¡å—
            freq_block1 = bbox_branch[0]  # FrequencyEnhancementBlock
            multi_scale = bbox_branch[2]   # MultiScaleFrequencyFusion
            
            if hasattr(freq_block1, 'alpha'):
                alpha = torch.sigmoid(freq_block1.alpha).item()
                stats[f'P{i+3}_bbox_freq_alpha'] = alpha
                stats[f'P{i+3}_bbox_freq_contrib'] = freq_block1.freq_contribution.item()
            
            if hasattr(multi_scale, 'beta'):
                beta = torch.sigmoid(multi_scale.beta).item()
                stats[f'P{i+3}_bbox_residual_beta'] = beta
                stats[f'P{i+3}_bbox_residual_contrib'] = multi_scale.residual_contribution.item()
            
            # åˆ†ç±»åˆ†æ”¯ - æœ‰1ä¸ªé¢‘ç‡å¢å¼ºæ¨¡å—
            freq_block2 = cls_branch[1]
            if hasattr(freq_block2, 'alpha'):
                alpha = torch.sigmoid(freq_block2.alpha).item()
                stats[f'P{i+3}_cls_freq_alpha'] = alpha
                stats[f'P{i+3}_cls_freq_contrib'] = freq_block2.freq_contribution.item()
        
        return stats


class Detect_WavFreq_Lite(nn.Module):
    """
    ğŸš€ WavFreq-Head v2.2 (Nano Edition)
    
    ğŸ“‰ æè‡´è½»é‡åŒ–è®¾è®¡:
    1. å…±äº«é¢‘ç‡å¢å¼º: Bboxå’ŒClså…±äº«åŒä¸€ä¸ªFEBï¼Œå‚æ•°å‡åŠ
    2. å¼ºåˆ¶é€šé“å‹ç¼©: è¾“å…¥å…ˆé™ç»´åˆ° hidden_dim (å¦‚256)ï¼Œé˜²æ­¢P5å±‚çˆ†ç‚¸
    3. æ·±åº¦å¯åˆ†ç¦»å·ç§¯: ç”¨ DWConv æ›¿æ¢éƒ¨åˆ† Conv
    
    ğŸ“Š å‚æ•°é‡å¯¹æ¯” (80ç±»):
    - åŸç‰ˆ v2.0 Lite: ~1.2M
    - æ­¤ç‰ˆæœ¬ Nano:    ~0.45M (æ¥è¿‘åŸç”Ÿ)
    """
    dynamic = False
    export = False
    shape = None
    anchors = torch.empty(0)
    strides = torch.empty(0)

    def __init__(self, nc=80, ch=(), wavelet='haar'):
        super().__init__()
        self.nc = nc
        self.nl = len(ch)
        self.reg_max = 16
        self.no = nc + self.reg_max * 4
        self.stride = torch.zeros(self.nl)
        
        # ğŸ”¥ å…³é”®ä¿®æ”¹1: ç»Ÿä¸€å†…éƒ¨é€šé“æ•°
        # ä¸ç®¡è¾“å…¥æ˜¯å¤šå°‘(256/512/1024)ï¼Œå†…éƒ¨ç»Ÿä¸€ç”¨ c_hid å¤„ç†
        # å¯¹äº Nano æ¨¡å‹ï¼Œ128 æˆ– 160 å°±å¤Ÿäº†ï¼›Tiny ç”¨ 192 æˆ– 256
        c_hid = max(64, min(ch[0], 256)) 
        
        # å…±äº«çš„ Stem å±‚ (é™ç»´ + é¢‘ç‡å¢å¼º)
        self.stems = nn.ModuleList()
        for x in ch:
            self.stems.append(nn.Sequential(
                # å…ˆé™ç»´! (1024 -> 256) è¿™æ˜¯çœå‚æ•°çš„å…³é”®
                Conv(x, c_hid, 1), 
                # åœ¨ä½ç»´ç©ºé—´åšé¢‘ç‡å¢å¼º (è®¡ç®—é‡å°)
                FrequencyEnhancementBlock(c_hid, wavelet), 
                # 3x3 å·ç§¯èåˆ
                Conv(c_hid, c_hid, 3)
            ))
        
        # è§£è€¦å¤´ (Decoupled Head) - åªæœ‰æœ€åçš„æŠ•å½±å±‚
        # ä¸å†é‡å¤å †å å·ç§¯ï¼Œå¤ç”¨ stem çš„ç‰¹å¾
        self.cv2 = nn.ModuleList([
            nn.Sequential(
                Conv(c_hid, c_hid, 3, g=c_hid), # DWConv çœå‚æ•°
                nn.Conv2d(c_hid, 4 * self.reg_max, 1)
            ) for _ in ch
        ])
        
        self.cv3 = nn.ModuleList([
            nn.Sequential(
                Conv(c_hid, c_hid, 3, g=c_hid), # DWConv çœå‚æ•°
                nn.Conv2d(c_hid, self.nc, 1)
            ) for _ in ch
        ])
        
        self.dfl = DFL(self.reg_max)
        
    def forward(self, x):
        shape = x[0].shape
        for i in range(self.nl):
            # 1. å…±äº«ç‰¹å¾æå– (å«é¢‘ç‡å¢å¼º)
            feat = self.stems[i](x[i])
            
            # 2. åˆ†æ”¯é¢„æµ‹
            box_out = self.cv2[i](feat)
            cls_out = self.cv3[i](feat)
            
            x[i] = torch.cat((box_out, cls_out), 1)
            
        if self.training:
            return x
            
        if self.dynamic or self.shape != shape:
            self.anchors, self.strides = (x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5))
            self.shape = shape
            
        x_cat = torch.cat([xi.view(shape[0], self.no, -1) for xi in x], 2)
        box, cls = x_cat.split((self.reg_max * 4, self.nc), 1)
        dbox = dist2bbox(self.dfl(box), self.anchors.unsqueeze(0), xywh=True, dim=1) * self.strides
        y = torch.cat((dbox, cls.sigmoid()), 1)
        return y if self.export else (y, x)

    def bias_init(self):
        for a, b, s in zip(self.cv2, self.cv3, self.stride):
            a[-1].bias.data[:] = 1.0
            b[-1].bias.data[:self.nc] = math.log(5 / self.nc / (640 / s) ** 2)

# ============================================================================
# 6. è¾…åŠ©å‡½æ•° - Shortcutæ£€æµ‹å’Œç›‘æ§
# ============================================================================

def test_shortcut_bypass(model, device='cpu'):
    """
    âœ… v2.0æ–°å¢: æµ‹è¯•æ¨¡å‹æ˜¯å¦çœŸçš„ä½¿ç”¨äº†é¢‘åŸŸæ¨¡å—
    
    æ–¹æ³•: ç ´åé¢‘åŸŸæ³¨æ„åŠ›æ¨¡å—ï¼Œçœ‹è¾“å‡ºæ˜¯å¦å˜åŒ–
    
    Args:
        model: æ£€æµ‹æ¨¡å‹ (æ•´ä¸ªYOLOæ¨¡å‹æˆ–åªæ˜¯head)
        device: 'cpu' or 'cuda'
    
    Returns:
        bool: Trueè¡¨ç¤ºçœŸçš„ç”¨äº†é¢‘åŸŸï¼ŒFalseè¡¨ç¤ºè¢«bypassäº†
        
    ä½¿ç”¨ç¤ºä¾‹:
        # åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®šæœŸæ£€æŸ¥
        if epoch % 10 == 0:
            is_working = test_shortcut_bypass(model.model[-1])  # model[-1]æ˜¯head
            if not is_working:
                print("âš ï¸ è­¦å‘Š: é¢‘åŸŸæ¨¡å—å¯èƒ½æ²¡åœ¨å·¥ä½œ!")
    """
    # æ‰¾åˆ°æ£€æµ‹å¤´
    if hasattr(model, 'model'):
        head = model.model[-1]  # YOLOçš„headæ˜¯æœ€åä¸€å±‚
    else:
        head = model
    
    # å‡†å¤‡æµ‹è¯•è¾“å…¥
    ch = [256, 512, 1024]  # å…¸å‹çš„é€šé“æ•°
    x_list = [
        torch.randn(1, ch[0], 80, 80).to(device),
        torch.randn(1, ch[1], 40, 40).to(device),
        torch.randn(1, ch[2], 20, 20).to(device)
    ]
    
    head.eval()
    
    # 1. æ­£å¸¸å‰å‘
    with torch.no_grad():
        output1 = head(x_list)
        if isinstance(output1, tuple):
            output1 = output1[0]
    
    # 2. ä¸´æ—¶æ›¿æ¢æ‰€æœ‰é¢‘åŸŸæ³¨æ„åŠ›æ¨¡å—ä¸ºIdentity
    original_modules = []
    for module in head.modules():
        if hasattr(module, 'freq_attn'):
            original_modules.append((module, 'freq_attn', module.freq_attn))
            module.freq_attn = nn.Identity()
    
    # 3. å†æ¬¡å‰å‘
    with torch.no_grad():
        output2 = head(x_list)
        if isinstance(output2, tuple):
            output2 = output2[0]
    
    # 4. æ¢å¤æ¨¡å—
    for module, attr_name, original_attr in original_modules:
        setattr(module, attr_name, original_attr)
    
    # 5. è®¡ç®—å·®å¼‚
    if isinstance(output1, list):
        diff = sum([(o1 - o2).abs().mean() for o1, o2 in zip(output1, output2)]) / len(output1)
    else:
        diff = (output1 - output2).abs().mean()
    
    diff = diff.item()
    
    print(f"\nğŸ” Shortcut Bypass Test:")
    print(f"  æ­£å¸¸è¾“å‡º vs ç ´åé¢‘åŸŸåçš„å·®å¼‚: {diff:.6f}")
    
    threshold = 1e-4
    if diff < threshold:
        print(f"  âŒ è­¦å‘Š: å·®å¼‚ < {threshold}ï¼Œé¢‘åŸŸæ¨¡å—å¯èƒ½è¢«bypass!")
        return False
    else:
        print(f"  âœ… å·®å¼‚æ˜¾è‘—ï¼Œé¢‘åŸŸæ¨¡å—æ­£å¸¸å·¥ä½œ!")
        return True


def frequency_utilization_loss(model):
    """
    âœ… v2.0æ–°å¢: è®¡ç®—é¢‘ç‡åˆ©ç”¨ç‡æŸå¤±
    
    é¼“åŠ±æ¨¡å‹çœŸæ­£ä½¿ç”¨é¢‘åŸŸä¿¡æ¯:
    - æƒ©ç½šalphaå¤ªæ¥è¿‘0æˆ–1 (å¸Œæœ›åœ¨0.3~0.7)
    - æƒ©ç½šbetaå¤ªå¤§ (å¸Œæœ›æ®‹å·®æƒé‡å°ï¼Œ<0.5)
    
    ä½¿ç”¨ç¤ºä¾‹:
        # åœ¨è®­ç»ƒå¾ªç¯ä¸­
        loss = compute_loss(pred, target)
        loss += frequency_utilization_loss(model) * 0.1  # 0.1æ˜¯æƒé‡
        loss.backward()
    
    Returns:
        torch.Tensor: æ ‡é‡æŸå¤±å€¼
    """
    penalty = 0.0
    count = 0
    
    for module in model.modules():
        # æ£€æŸ¥FrequencyEnhancementBlock
        if hasattr(module, 'alpha'):
            alpha = torch.sigmoid(module.alpha)
            # æƒ©ç½šalphaåç¦»0.5 (å¸Œæœ›åœ¨0.3~0.7)
            penalty += torch.abs(alpha - 0.5)
            count += 1
        
        # æ£€æŸ¥MultiScaleFrequencyFusion
        if hasattr(module, 'beta'):
            beta = torch.sigmoid(module.beta)
            # æƒ©ç½šbetaå¤ªå¤§ (å¸Œæœ›æ®‹å·®å°)
            penalty += beta
            count += 1
    
    if count == 0:
        return torch.tensor(0.0)
    
    return penalty / count


def print_frequency_stats(model, logger=None):
    """
    âœ… v2.0æ–°å¢: æ‰“å°é¢‘ç‡åˆ©ç”¨ç‡ç»Ÿè®¡
    
    ä½¿ç”¨ç¤ºä¾‹:
        # æ¯ä¸ªepochç»“æŸå
        print_frequency_stats(model)
        
        # æˆ–ä½¿ç”¨logger
        print_frequency_stats(model, logger=wandb)
    """
    if hasattr(model, 'model'):
        head = model.model[-1]
    else:
        head = model
    
    if not hasattr(head, 'get_frequency_utilization'):
        print("âš ï¸ æ¨¡å‹æ²¡æœ‰frequency utilizationç›‘æ§åŠŸèƒ½")
        return
    
    stats = head.get_frequency_utilization()
    
    print("\n" + "="*60)
    print("ğŸ“Š Frequency Utilization Statistics")
    print("="*60)
    
    for key, value in stats.items():
        status = "âœ…" if 0.2 < value < 0.8 else "âš ï¸"
        print(f"{status} {key:30s}: {value:.4f}")
    
    print("="*60)
    print("ğŸ’¡ å»ºè®®:")
    print("  - alphaåº”åœ¨0.3~0.7ä¹‹é—´ (é¢‘åŸŸè´¡çŒ®åº¦)")
    print("  - betaåº”<0.5 (æ®‹å·®ä¸åº”ä¸»å¯¼)")
    print("  - å¦‚æœalphaæ¥è¿‘0ï¼Œè¯´æ˜é¢‘åŸŸæ¨¡å—æ²¡è¢«ä½¿ç”¨!")
    print("="*60 + "\n")
    
    # å¦‚æœæœ‰loggerï¼Œä¹Ÿè®°å½•åˆ°wandb/tensorboard
    if logger is not None:
        for key, value in stats.items():
            logger.log({f"freq_util/{key}": value})


# ============================================================================
# å¯¼å‡º
# ============================================================================

__all__ = [
    'WaveletDecompose',
    'WaveletReconstruct',
    'FrequencyAttention',
    'FrequencyEnhancementBlock',
    'WaveletDownsample',
    'MultiScaleFrequencyFusion',
    'Detect_WavFreq',
    'Detect_WavFreq_Lite',
    'test_shortcut_bypass',
    'frequency_utilization_loss',
    'print_frequency_stats',
]


# ============================================================================
# æµ‹è¯•ä»£ç 
# ============================================================================

if __name__ == "__main__":
    print("=" * 80)
    print("ğŸŒŠ Testing WavFreq Detection Head v2.0")
    print("=" * 80)
    
    # æµ‹è¯•å®Œæ•´ç‰ˆ
    print("\nâœ“ Testing Detect_WavFreq...")
    model = Detect_WavFreq(nc=80, ch=(256, 512, 1024))
    model.stride = torch.tensor([8., 16., 32.])
    model.train()
    
    x_list = [
        torch.randn(2, 256, 80, 80),
        torch.randn(2, 512, 40, 40),
        torch.randn(2, 1024, 20, 20)
    ]
    
    outputs = model(x_list)
    print(f"  Output scales: {[o.shape for o in outputs]}")
    
    # æ£€æŸ¥é¢‘ç‡åˆ©ç”¨ç‡
    print("\nâœ“ Checking frequency utilization...")
    stats = model.get_frequency_utilization()
    for key, value in sorted(stats.items()):
        status = "âœ…" if 0.2 < value < 0.8 else "âš ï¸"
        print(f"  {status} {key}: {value:.4f}")
    
    # Shortcutæµ‹è¯•
    print("\nâœ“ Testing for shortcut bypass...")
    is_working = test_shortcut_bypass(model, device='cpu')
    
    # æµ‹è¯•è½»é‡ç‰ˆ
    print("\nâœ“ Testing Detect_WavFreq_Lite...")
    model_lite = Detect_WavFreq_Lite(nc=80, ch=(256, 512, 1024))
    model_lite.stride = torch.tensor([8., 16., 32.])
    model_lite.train()
    
    outputs_lite = model_lite(x_list)
    print(f"  Output scales: {[o.shape for o in outputs_lite]}")
    
    stats_lite = model_lite.get_frequency_utilization()
    print(f"  Frequency stats: {stats_lite}")
    
    # å‚æ•°ç»Ÿè®¡
    def count_parameters(m):
        return sum(p.numel() for p in m.parameters() if p.requires_grad)
    
    print("\nğŸ“Š Parameter Stats:")
    print(f"  Detect_WavFreq:      {count_parameters(model):,} params")
    print(f"  Detect_WavFreq_Lite: {count_parameters(model_lite):,} params")
    print(f"  Reduction:           {(1 - count_parameters(model_lite)/count_parameters(model))*100:.1f}%")
    
    print("\n" + "=" * 80)
    print("âœ… All tests passed!")
    print("=" * 80)
    
    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("1. è®­ç»ƒæ—¶æ·»åŠ : loss += frequency_utilization_loss(model)")
    print("2. æ¯10ä¸ªepochæ£€æŸ¥: test_shortcut_bypass(model)")
    print("3. æ¯ä¸ªepochç»“æŸ: print_frequency_stats(model)")
    print("4. ç›‘æ§alphaå€¼ï¼Œç¡®ä¿åœ¨0.3~0.7ä¹‹é—´")
    print("\nğŸ“ YAMLé…ç½®:")
    print("head:")
    print("  - [[P3, P4, P5], 1, Detect_WavFreq, [nc]]  # å®Œæ•´ç‰ˆ")
    print("  # æˆ–")
    print("  - [[P3, P4, P5], 1, Detect_WavFreq_Lite, [nc]]  # è½»é‡ç‰ˆ")